{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all necesssary packages\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pickle\n",
    "from os.path import expanduser\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "from torch.utils import data\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#some necessary helper functions\n",
    "def load_txt(txt_path):\n",
    "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def save_txt(text, save_path):\n",
    "    with open(save_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(text)\n",
    "\n",
    "def load_pkl(file_path):\n",
    "    with open(file_path, 'rb') as pkl_file:\n",
    "        content = pickle.load(pkl_file)\n",
    "    return content\n",
    "\n",
    "def save_pkl(content, folder_path, file_name):\n",
    "    path = os.path.join(folder_path, file_name)\n",
    "    with open(path, 'wb') as pkl_file:\n",
    "        pickle.dump(content, pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load both benign and malware datasets (fasttext dataset) and build the dataset\n",
    "benign_pkls_list = 'tensors/i386_train'\n",
    "malware_pkls_list = 'tensors/i386_malware'\n",
    "\n",
    "benign_pkls_path_list = [os.path.join(benign_pkls_list, file_name) for file_name in os.listdir(benign_pkls_list)]\n",
    "malware_pkls_path_list = [os.path.join(malware_pkls_list, file_name) for file_name in os.listdir(malware_pkls_list)]\n",
    "\n",
    "# benign_pkls_path_list = random.sample(benign_pkls_path_list, 10)\n",
    "# malware_pkls_path_list = random.sample(malware_pkls_path_list, 10)\n",
    "\n",
    "x86_training_labels = [0] * len(benign_pkls_path_list) + [1] * len(malware_pkls_path_list)\n",
    "x86_training_dataset = benign_pkls_path_list + malware_pkls_path_list\n",
    "\n",
    "# for path in benign_pkls_path_list:\n",
    "#     print(load_pkl(path).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0]\n",
      "280\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x86_training_dataset, x86_training_labels, test_size=0.2, random_state=42)\n",
    "print(y_test)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def data_generator(X, y, batch_size):\n",
    "    n_batches = int(np.ceil(len(y) / batch_size))\n",
    "    \n",
    "    for i in range(n_batches):\n",
    "        start = i * batch_size\n",
    "        end = min((i + 1) * batch_size, len(y))\n",
    "        \n",
    "        current_X_paths = X[start:end]\n",
    "        loaded_X_data = [load_pkl(file) for file in current_X_paths]\n",
    "        # print(f\"file loaded: {file}\")\n",
    "        # loaded_X_data = []\n",
    "        # for file in current_X_paths:\n",
    "        #     loaded_X_data.append(load_pkl(file))\n",
    "        #     print(f\"file loaded: {file}\") \n",
    "        X_batch = torch.stack(loaded_X_data).float()\n",
    "        y_batch = torch.from_numpy(np.array(y[start:end])).float()\n",
    "        \n",
    "        yield X_batch, y_batch\n",
    "\n",
    "# for inputs, targets in data_generator(x86_training_dataset, x86_training_labels):\n",
    "#     print(\"inputs\", inputs, \"shape: \", inputs.shape)\n",
    "#     print(\"targets\", targets, \"shape: \", targets.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RNN(nn.Module):\n",
    "    # embed_dim is 100 in our case\n",
    "    # n_hidden: the dimension of each hidden layer\n",
    "    # n_rnnlayers: the number of layers\n",
    "    def __init__(self, embed_dim, n_hidden, n_rnnlayers, n_outputs):\n",
    "        super(RNN, self).__init__()\n",
    "        self.D = embed_dim\n",
    "        self.M = n_hidden\n",
    "        self.K = n_outputs\n",
    "        self.L = n_rnnlayers\n",
    "        \n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = self.D,\n",
    "            hidden_size = self.M, \n",
    "            num_layers = self.L,\n",
    "            batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.M, self.K)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        # initial hidden states\n",
    "        h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "        c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "\n",
    " \n",
    "        # get RNN unit output\n",
    "        out, _ = self.rnn(X, (h0, c0))\n",
    "        \n",
    "        # max pool\n",
    "        out, _ = torch.max(out, 1)\n",
    "        \n",
    "        # we only want h(T) at the final time step\n",
    "        out = self.fc(out)\n",
    "        return out   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (rnn): LSTM(24, 8, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=8, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_dim = 24\n",
    "n_hidden = 8\n",
    "n_rnnlayers = 2\n",
    "n_outputs = 1\n",
    "batch_size = 10\n",
    "model = RNN(embed_dim, n_hidden, n_rnnlayers, n_outputs)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = lambda: data_generator(X_train, y_train, batch_size)\n",
    "test_gen = lambda: data_generator(X_test, y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gd(model, criterion, optimizer, epochs):\n",
    "    train_losses = np.zeros(epochs)\n",
    "    train_accs = np.zeros(epochs)\n",
    "    \n",
    "    test_losses = np.zeros(epochs) \n",
    "    test_accs = np.zeros(epochs)\n",
    "    \n",
    "    test_arm_losses = np.zeros(epochs)\n",
    "    test_arm_accs = np.zeros(epochs)\n",
    "    \n",
    "    for it in range(epochs):\n",
    "        t0 = datetime.now()\n",
    "        train_loss = []   \n",
    "        n_correct = 0.\n",
    "        n_total = 0.\n",
    "        for inputs, targets in tqdm(train_gen(), desc=f\"Epoch {it+1}/{epochs} - Training\"):\n",
    "            targets = targets.view(-1, 1).float()\n",
    "            # move data to GPU\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            predictions = (outputs > 0)\n",
    "            n_correct += (predictions == targets).sum().item()\n",
    "            n_total += targets.shape[0]\n",
    "\n",
    "            train_loss.append(loss.item())   \n",
    "        train_loss = np.mean(train_loss)\n",
    "        train_acc = n_correct / n_total\n",
    "        \n",
    "        \n",
    "        # Get test loss and acc\n",
    "        n_correct = 0.\n",
    "        n_total = 0.\n",
    "        test_loss = []\n",
    "        for inputs, targets in tqdm(test_gen(), desc=f\"Epoch {it+1}/{epochs} - Testing\"):  # Added tqdm here for testing\n",
    "            targets = targets.view(-1, 1).float()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss.append(loss.item())\n",
    "            \n",
    "            predictions = (outputs > 0) \n",
    "            n_correct += (predictions == targets).sum().item()\n",
    "            n_total += targets.shape[0]  \n",
    "        test_loss = np.mean(test_loss)\n",
    "        test_acc = n_correct / n_total\n",
    "\n",
    "        # save everything!!!\n",
    "        train_losses[it] = train_loss\n",
    "        test_losses[it] = test_loss\n",
    "        train_accs[it] = train_acc\n",
    "        test_accs[it] = test_acc\n",
    "        # test_arm_losses[it] = test_arm_loss\n",
    "        # test_arm_accs[it] = test_arm_acc\n",
    "                \n",
    "        dt = datetime.now() - t0\n",
    "#         if it % 100 == 0:\n",
    "#         torch.save(model.state_dict(), 'lstm100_it{}.pt'.format(it))\n",
    "        print(f'Epoch {it}/{epochs},  Duration: {dt}')\n",
    "        print(f'Train Loss: {train_loss:.4f},     Train Acc: {train_acc:.4f}')\n",
    "        print(f'Test Loss: {test_loss:.4f},       Test Acc: {test_acc:.4f}')\n",
    "        # print(f'Test ARM Loss: {test_arm_loss:.4f},       Test ARM ACC: {test_arm_acc:.4f}') \n",
    "        save_path = f'x86_training_result/{embed_dim}_{n_hidden}_{n_rnnlayers}_{n_outputs}'\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        filename = 'lstm_it{}.pt'.format(it)\n",
    "        torch.save(model.state_dict(), os.path.join(save_path, filename))\n",
    "    return train_losses, test_losses, train_accs, test_accs, test_arm_losses, test_arm_accs  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Training: 112it [11:54,  6.38s/it]\n",
      "Epoch 1/20 - Testing: 28it [02:06,  4.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20,  Duration: 0:14:00.815858\n",
      "Train Loss: 0.6896,     Train Acc: 0.5000\n",
      "Test Loss: 0.6790,       Test Acc: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20 - Training: 112it [12:13,  6.55s/it]\n",
      "Epoch 2/20 - Testing: 28it [02:06,  4.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20,  Duration: 0:14:19.536371\n",
      "Train Loss: 0.6397,     Train Acc: 0.7179\n",
      "Test Loss: 0.5818,       Test Acc: 0.7821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20 - Training: 112it [12:12,  6.54s/it]\n",
      "Epoch 3/20 - Testing: 28it [02:06,  4.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20,  Duration: 0:14:19.086642\n",
      "Train Loss: 0.5423,     Train Acc: 0.7955\n",
      "Test Loss: 0.5114,       Test Acc: 0.7821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20 - Training: 112it [12:09,  6.52s/it]\n",
      "Epoch 4/20 - Testing: 28it [02:06,  4.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20,  Duration: 0:14:15.850179\n",
      "Train Loss: 0.5198,     Train Acc: 0.7330\n",
      "Test Loss: 0.4597,       Test Acc: 0.7821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20 - Training: 112it [12:18,  6.60s/it]\n",
      "Epoch 5/20 - Testing: 28it [02:08,  4.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20,  Duration: 0:14:27.684728\n",
      "Train Loss: 0.4497,     Train Acc: 0.7661\n",
      "Test Loss: 0.4112,       Test Acc: 0.8929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20 - Training: 112it [12:11,  6.53s/it]\n",
      "Epoch 6/20 - Testing: 28it [02:06,  4.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20,  Duration: 0:14:18.042227\n",
      "Train Loss: 0.3706,     Train Acc: 0.9187\n",
      "Test Loss: 0.3477,       Test Acc: 0.9393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20 - Training: 112it [12:28,  6.68s/it]\n",
      "Epoch 7/20 - Testing: 28it [02:06,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20,  Duration: 0:14:34.976640\n",
      "Train Loss: 0.3967,     Train Acc: 0.8277\n",
      "Test Loss: 0.3384,       Test Acc: 0.9393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20 - Training: 112it [12:13,  6.54s/it]\n",
      "Epoch 8/20 - Testing: 28it [02:08,  4.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20,  Duration: 0:14:21.609811\n",
      "Train Loss: 0.3108,     Train Acc: 0.9366\n",
      "Test Loss: 0.3088,       Test Acc: 0.9357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20 - Training: 112it [12:14,  6.56s/it]\n",
      "Epoch 9/20 - Testing: 28it [02:04,  4.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20,  Duration: 0:14:18.940083\n",
      "Train Loss: 0.2861,     Train Acc: 0.9384\n",
      "Test Loss: 0.2839,       Test Acc: 0.9321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20 - Training: 26it [02:57,  6.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_losses, test_losses, train_accs, test_accs, test_arm_losses, test_arm_accs  \u001b[39m=\u001b[39m batch_gd(model, criterion, optimizer, \u001b[39m20\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[23], line 29\u001b[0m, in \u001b[0;36mbatch_gd\u001b[0;34m(model, criterion, optimizer, epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     28\u001b[0m \u001b[39m# backward and optimize\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     32\u001b[0m predictions \u001b[39m=\u001b[39m (outputs \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/lstm/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/lstm/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_losses, test_losses, train_accs, test_accs, test_arm_losses, test_arm_accs  = batch_gd(model, criterion, optimizer, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_correct = 0.\n",
    "# n_total = 0.\n",
    "# test_loss = []\n",
    "# for inputs, targets in test_gen():\n",
    "#     targets = targets.view(-1, 1).float()\n",
    "#     inputs, targets = inputs.to(device), targets.to(device)\n",
    "#     outputs = model(inputs)\n",
    "#     loss = criterion(outputs, targets)\n",
    "#     test_loss.append(loss.item())\n",
    "    \n",
    "#     predictions = (outputs > 0) \n",
    "#     n_correct += (predictions == targets).sum().item()\n",
    "#     # print(n_correct)\n",
    "#     n_total += targets.shape[0]  \n",
    "#     print(\"=====targets=====\")\n",
    "#     print(targets)\n",
    "#     print(\"=====outputs=====\")\n",
    "#     print(outputs)\n",
    "# test_loss = np.mean(test_loss)\n",
    "# test_acc = n_correct / n_total\n",
    "# print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test = [load_pkl(file_path) for file_path in X_test]\n",
    "\n",
    "# print(X_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.metrics as metrics\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Define a batch generator\n",
    "# def batched_data_loader(file_paths, labels, batch_size=10):\n",
    "#     for i in range(0, len(file_paths), batch_size):\n",
    "#         batch_files = file_paths[i:i+batch_size]\n",
    "#         batch_data = [load_pkl(file) for file in batch_files]\n",
    "#         yield torch.stack(batch_data), torch.tensor(labels[i:i+batch_size])\n",
    "\n",
    "# # Parameters\n",
    "# batch_size = 20\n",
    "# all_probs = []\n",
    "\n",
    "# # Process in batches\n",
    "# for inputs, targets in batched_data_loader(X_test, y_test, batch_size):\n",
    "#     inputs, targets = inputs.to(device), targets.to(device)\n",
    "#     batch_output = model(inputs)\n",
    "#     batch_probs = torch.sigmoid(batch_output)\n",
    "#     all_probs.append(batch_probs.detach().cpu().numpy())\n",
    "\n",
    "# # Concatenate all outputs\n",
    "# mips_probs = np.concatenate(all_probs, axis=0)\n",
    "# # print(f\"mips_probs {mips_probs}\")\n",
    "# # Now, you can calculate fpr, tpr, etc.\n",
    "# mips_preds = mips_probs[:,0]\n",
    "# fpr_mips, tpr_mips, threshold_mips = metrics.roc_curve(y_test, mips_preds)\n",
    "# roc_auc_mips = metrics.auc(fpr_mips, tpr_mips)\n",
    "\n",
    "# # method I: plt\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.title('Receiver Operating Characteristic for mips Data')\n",
    "# plt.plot(fpr_mips, tpr_mips, 'b', label = 'AUC = %0.2f' % roc_auc_mips)\n",
    "# plt.legend(loc = 'lower right')\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0, 1])\n",
    "# plt.ylim([0, 1])\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn.metrics as metrics\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def get_model_predictions(model, X, device, batch_size=50):\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     all_preds = []\n",
    "\n",
    "#     # Loop over the dataset in mini-batches\n",
    "#     for i in range(0, len(X), batch_size):\n",
    "#         batch_X = X[i:i+batch_size]\n",
    "#         X_tensor = torch.stack(batch_X).to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(X_tensor)\n",
    "#             probs = torch.sigmoid(outputs)\n",
    "#             preds = probs[:, 0].cpu().numpy()\n",
    "\n",
    "#         all_preds.extend(preds)\n",
    "\n",
    "#     return np.array(all_preds)\n",
    "\n",
    "# # Fetch model predictions using the function\n",
    "# preds = get_model_predictions(model, X_test, device)\n",
    "# print(len(preds))\n",
    "# # Compute ROC curve and AUC\n",
    "# fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "# roc_auc = metrics.auc(fpr, tpr)\n",
    "# print(roc_auc)\n",
    "# # Plotting the ROC curve\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "# plt.legend(loc = 'lower right')\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0, 1])\n",
    "# plt.ylim([0, 1])\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import sklearn.metrics as metrics\n",
    "\n",
    "# X_test_tensor = torch.stack(X_test).to(device)\n",
    "# outputs = model(X_test_tensor)\n",
    "# probs = torch.sigmoid(outputs)\n",
    "# preds = probs[:,0].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "# # Compute ROC curve and AUC\n",
    "# fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "# roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# # method I: plt\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.title('Receiver Operating Characteristic')\n",
    "# plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "# plt.legend(loc = 'lower right')\n",
    "# plt.plot([0, 1], [0, 1],'r--')\n",
    "# plt.xlim([0, 1])\n",
    "# plt.ylim([0, 1])\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lstm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
